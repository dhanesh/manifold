# non-programming-evaluation

## Outcome

Evaluate manifold for non-programming problem solving

---

## Context

### Motivation

- Manifold was designed for software engineering constraints
- Constraint-first thinking may apply to broader decision-making
- Need to validate applicability beyond code

### Prior Art

- Decision matrices and weighted scoring
- Pro/con lists and Franklin squares
- SWOT analysis for business decisions
- Design thinking and jobs-to-be-done frameworks

### Success Metrics

- Identify which constraint categories translate to non-programming
- Document where the framework needs adaptation
- Produce actionable recommendations for each domain

---

## Constraints

### Business

#### B1: Evaluation must cover all 4 problem domains (business, personal, creative, research)

Evaluation must cover all 4 problem domains (business, personal, creative, research)

> **Rationale:** Comprehensive validation requires testing across diverse problem types

#### B2: Findings must be actionable, not just observational

Findings must be actionable, not just observational

> **Rationale:** Goal is adaptations, not just a verdict

#### B3: Identify reusable patterns across domains

Identify reusable patterns across domains

> **Rationale:** Cross-domain applicability increases framework value

### Technical

#### T1: Each domain must be tested with at least one hypothetical scenario

Each domain must be tested with at least one hypothetical scenario

> **Rationale:** Hypothetical testing chosen as evaluation method

#### T2: Constraint categories (B/T/U/S/O) must map to non-programming equivalents

Constraint categories (B/T/U/S/O) must map to non-programming equivalents

> **Rationale:** Core framework structure must translate or be adapted

#### T3: Evaluation limited to constraint discovery and tension analysis phases

Evaluation limited to constraint discovery and tension analysis phases

> **Rationale:** Generate/verify phases are code-specific; focus on conceptual phases

#### T4: Document which /m commands are applicable vs need adaptation

Document which /m commands are applicable vs need adaptation

> **Rationale:** Actionable output for framework evolution

### User Experience

#### U1: Non-technical users should understand the adapted framework

Non-technical users should understand the adapted framework

> **Rationale:** Programming jargon may alienate non-developer users

#### U2: Constraint discovery should take < 30 minutes per problem

Constraint discovery should take < 30 minutes per problem

> **Rationale:** Complex decisions shouldn't require multi-hour sessions

#### U3: Output should aid decision-making, not just document constraints

Output should aid decision-making, not just document constraints

> **Rationale:** Practical value over theoretical completeness

### Security

#### S1: Personal decisions should allow private/local-only manifolds

Personal decisions should allow private/local-only manifolds

> **Rationale:** Life decisions may contain sensitive information

#### S2: No personal data required for hypothetical scenarios

No personal data required for hypothetical scenarios

> **Rationale:** Testing with hypotheticals avoids privacy concerns

### Operational

#### O1: Document template manifolds for each validated domain

Document template manifolds for each validated domain

> **Rationale:** Reusable templates lower adoption barrier

#### O2: Evaluation completable in single session

Evaluation completable in single session

> **Rationale:** Meta-evaluation shouldn't require multi-day effort

#### O3: Produce README section for non-programming use cases

Produce README section for non-programming use cases

> **Rationale:** Findings should be documented for future users

---

## Tensions

### TN1: Testing all 4 domains thoroughly conflicts with completing in single session

Testing all 4 domains thoroughly conflicts with completing in single session

> **Resolution:** Use lightweight hypothetical scenarios (5-10 min each), not deep dives. Total ~40 min acceptable given meta-evaluation nature.

### TN2: Limiting to discovery/tension phases may reduce actionable findings for generate/verify

Limiting to discovery/tension phases may reduce actionable findings for generate/verify

> **Resolution:** Accept limitation. Generate/verify are code-specific by design. Actionable findings focus on category mappings and terminology adaptations.

### TN3: Category names (Technical, Operational, Security) are programming-centric; non-technical users may not relate

Category names (Technical, Operational, Security) are programming-centric; non-technical users may not relate

> **Resolution:** Propose category renaming for non-programming: Business→Goals, Technical→Feasibility, UX→Experience, Security→Risks, Operational→Logistics

### TN4: Cannot create domain templates (O1) without first testing each domain (T1)

Cannot create domain templates (O1) without first testing each domain (T1)

> **Resolution:** T1 is prerequisite for O1. Execution order: test domains first, then extract templates.

### TN5: 30-min limit per problem (U2) × 4 domains (B1) = 2 hours minimum, conflicts with 'single session'

30-min limit per problem (U2) × 4 domains (B1) = 2 hours minimum, conflicts with 'single session'

> **Resolution:** U2 applies to real usage, not this evaluation. Evaluation can take longer since it's a one-time meta-analysis.

---

## Required Truths

### RT-1: Each of the 4 domains must be tested with a representative scenario

Each of the 4 domains must be tested with a representative scenario

### RT-2: Constraint categories must translate or have clear non-programming mappings

Constraint categories must translate or have clear non-programming mappings

### RT-3: Tensions between constraints must surface naturally from non-programming scenarios

Tensions between constraints must surface naturally from non-programming scenarios

### RT-4: Findings must be concrete and actionable

Findings must be concrete and actionable

### RT-5: The adapted framework must be understandable to non-technical users

The adapted framework must be understandable to non-technical users

### RT-6: Evaluation methodology must be efficient and repeatable

Evaluation methodology must be efficient and repeatable
